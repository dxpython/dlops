#!/usr/bin/env python3
"""
DLOps AI-Driven Backend Server
Developed by: Charles 
"""

import json
import os
import subprocess
import time
import threading
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from flask import Flask, jsonify, request, Response, send_from_directory
import logging
from collections import defaultdict, deque

# é…ç½®
DLOPS_WORKDIR = os.environ.get('DLOPS_WORKDIR', f"{os.path.expanduser('~')}/.dlops")
WEB_PORT = int(os.environ.get('WEB_PORT', 8080))
DEBUG_MODE = os.environ.get('DEBUG', 'false').lower() == 'true'
STATIC_DIR = os.environ.get('STATIC_DIR', './static')  # HTMLæ–‡ä»¶ç›®å½•

app = Flask(__name__)
app.secret_key = 'dlops_ai_secret_2024'

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€çŠ¶æ€
metrics_cache = deque(maxlen=100)
anomaly_detector = None
performance_analyzer = None

class AnomalyDetector:
    """AIå¼‚å¸¸æ£€æµ‹å¼•æ“"""
    def __init__(self):
        self.baseline_metrics = {}
        self.anomaly_threshold = 2.0  # æ ‡å‡†å·®å€æ•°
        self.learning_window = 50
        
    def update_baseline(self, metrics):
        """æ›´æ–°åŸºçº¿æŒ‡æ ‡"""
        try:
            for key, value in self._flatten_metrics(metrics).items():
                if isinstance(value, (int, float)):
                    if key not in self.baseline_metrics:
                        self.baseline_metrics[key] = deque(maxlen=self.learning_window)
                    self.baseline_metrics[key].append(value)
        except Exception as e:
            logger.error(f"æ›´æ–°åŸºçº¿å¤±è´¥: {e}")
    
    def detect_anomalies(self, metrics):
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        try:
            flat_metrics = self._flatten_metrics(metrics)
            
            for key, value in flat_metrics.items():
                if (isinstance(value, (int, float)) and 
                    key in self.baseline_metrics and 
                    len(self.baseline_metrics[key]) > 10):
                    
                    baseline = list(self.baseline_metrics[key])
                    mean_val = np.mean(baseline)
                    std_val = np.std(baseline)
                    
                    if std_val > 0:
                        z_score = abs(value - mean_val) / std_val
                        if z_score > self.anomaly_threshold:
                            anomalies.append({
                                'metric': key,
                                'value': value,
                                'baseline_mean': mean_val,
                                'z_score': z_score,
                                'severity': 'high' if z_score > 3 else 'medium'
                            })
        except Exception as e:
            logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            
        return anomalies
    
    def _flatten_metrics(self, metrics, prefix=''):
        """å±•å¹³åµŒå¥—æŒ‡æ ‡"""
        flat = {}
        try:
            for key, value in metrics.items():
                new_key = f"{prefix}.{key}" if prefix else key
                if isinstance(value, dict):
                    flat.update(self._flatten_metrics(value, new_key))
                elif isinstance(value, (list, tuple)) and len(value) > 0:
                    if isinstance(value[0], (int, float)):
                        flat[new_key] = value[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
                elif isinstance(value, str):
                    # å°è¯•è§£ææ•°å­—å­—ç¬¦ä¸²
                    try:
                        if ',' in value:
                            # å¤„ç† "used,total,percentage" æ ¼å¼
                            parts = value.split(',')
                            if len(parts) >= 3:
                                flat[f"{new_key}.used"] = float(parts[0])
                                flat[f"{new_key}.total"] = float(parts[1])
                                flat[f"{new_key}.percent"] = float(parts[2])
                        else:
                            flat[new_key] = float(value)
                    except ValueError:
                        pass
                else:
                    flat[new_key] = value
        except Exception as e:
            logger.error(f"æŒ‡æ ‡å±•å¹³å¤±è´¥: {e}")
        return flat

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    def __init__(self):
        self.performance_history = deque(maxlen=200)
        self.training_patterns = {}
        
    def analyze_training_efficiency(self, metrics):
        """åˆ†æè®­ç»ƒæ•ˆç‡"""
        analysis = {
            'gpu_efficiency': 0,
            'memory_efficiency': 0,
            'io_efficiency': 0,
            'overall_score': 0,
            'bottlenecks': [],
            'recommendations': []
        }
        
        try:
            # GPUæ•ˆç‡åˆ†æ
            if 'gpu' in metrics and 'utilization' in metrics['gpu']:
                gpu_util = metrics['gpu']['utilization'][0] if isinstance(metrics['gpu']['utilization'], list) else metrics['gpu']['utilization']
                analysis['gpu_efficiency'] = gpu_util
                
                if gpu_util < 70:
                    analysis['bottlenecks'].append('GPUåˆ©ç”¨ç‡ä½')
                    analysis['recommendations'].append('å¢åŠ batch sizeæˆ–ä¼˜åŒ–æ•°æ®åŠ è½½')
                elif gpu_util > 95:
                    analysis['recommendations'].append('GPUåˆ©ç”¨ç‡è‰¯å¥½')
            
            # å†…å­˜æ•ˆç‡åˆ†æ
            if 'system' in metrics and 'memory' in metrics['system']:
                memory_info = metrics['system']['memory']
                if isinstance(memory_info, str) and ',' in memory_info:
                    parts = memory_info.split(',')
                    if len(parts) >= 3:
                        mem_percent = float(parts[2])
                        analysis['memory_efficiency'] = min(100, mem_percent * 1.2)  # è°ƒæ•´åˆ†æ•°
                        
                        if mem_percent > 90:
                            analysis['bottlenecks'].append('å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜')
                            analysis['recommendations'].append('å¯ç”¨gradient checkpointingæˆ–å‡å°‘batch size')
            
            # I/Oæ•ˆç‡åˆ†æï¼ˆåŸºäºCPU waæ—¶é—´æ¨æ–­ï¼‰
            analysis['io_efficiency'] = 85  
            
            # æ•´ä½“åˆ†æ•°è®¡ç®—
            analysis['overall_score'] = (
                analysis['gpu_efficiency'] * 0.5 +
                analysis['memory_efficiency'] * 0.3 +
                analysis['io_efficiency'] * 0.2
            )
            
        except Exception as e:
            logger.error(f"æ€§èƒ½åˆ†æå¤±è´¥: {e}")
            
        return analysis
    
    def predict_performance_trends(self):
        """é¢„æµ‹æ€§èƒ½è¶‹åŠ¿"""
        predictions = []
        
        if len(self.performance_history) < 10:
            return predictions
        
        try:
            # åˆ†æGPUåˆ©ç”¨ç‡è¶‹åŠ¿
            gpu_utils = [h.get('gpu_efficiency', 0) for h in self.performance_history if h.get('gpu_efficiency')]
            if len(gpu_utils) >= 5:
                recent_trend = np.mean(gpu_utils[-5:]) - np.mean(gpu_utils[-10:-5])
                if recent_trend < -10:
                    predictions.append({
                        'type': 'performance_degradation',
                        'message': 'GPUåˆ©ç”¨ç‡å‘ˆä¸‹é™è¶‹åŠ¿',
                        'confidence': min(90, abs(recent_trend) * 5),
                        'time_window': '5åˆ†é’Ÿ'
                    })
                    
        except Exception as e:
            logger.error(f"æ€§èƒ½é¢„æµ‹å¤±è´¥: {e}")
            
        return predictions

def run_dlops_command(command_args):
    """æ‰§è¡ŒDLOpså‘½ä»¤"""
    try:
        dlops_script = os.path.join(DLOPS_WORKDIR, '../../../dlops')
        if not os.path.exists(dlops_script):
            dlops_script = './dlops'
            
        result = subprocess.run([dlops_script] + command_args, 
                              capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            try:
                return json.loads(result.stdout) if result.stdout.strip() else {}
            except json.JSONDecodeError:
                return {'raw_output': result.stdout}
        else:
            return {'error': result.stderr, 'returncode': result.returncode}
            
    except subprocess.TimeoutExpired:
        return {'error': 'å‘½ä»¤è¶…æ—¶', 'returncode': -1}
    except Exception as e:
        return {'error': str(e), 'returncode': -1}

def get_latest_metrics():
    """è·å–æœ€æ–°æŒ‡æ ‡"""
    try:
        metrics_file = os.path.join(DLOPS_WORKDIR, 'metrics', 'latest.json')
        if os.path.exists(metrics_file):
            with open(metrics_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # æ›´æ–°ç¼“å­˜
                metrics_cache.append(data)
                
                # æ›´æ–°å¼‚å¸¸æ£€æµ‹åŸºçº¿
                if anomaly_detector:
                    anomaly_detector.update_baseline(data)
                
                return data
        return {'error': 'æŒ‡æ ‡æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œ'}
    except Exception as e:
        return {'error': f'è¯»å–æŒ‡æ ‡å¤±è´¥: {str(e)}'}

def get_diagnostics():
    """è·å–è¯Šæ–­ä¿¡æ¯"""
    try:
        diagnostics_dir = os.path.join(DLOPS_WORKDIR, 'diagnostics')
        latest_file = os.path.join(diagnostics_dir, 'latest_analysis.json')
        
        if os.path.exists(latest_file):
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # å¦‚æœæ²¡æœ‰è¯Šæ–­æ–‡ä»¶ï¼Œè¿è¡Œå¿«é€Ÿè¯Šæ–­
        return run_dlops_command(['analyze'])
        
    except Exception as e:
        return {'error': f'è·å–è¯Šæ–­ä¿¡æ¯å¤±è´¥: {str(e)}'}

def get_intelligent_analysis():
    """è·å–æ™ºèƒ½åˆ†æ"""
    analysis = {
        'anomalies': [],
        'performance': {},
        'predictions': [],
        'optimization_score': 0,
        'health_trend': 'stable'
    }
    
    try:
        # å¼‚å¸¸æ£€æµ‹
        if metrics_cache and anomaly_detector:
            latest_metrics = metrics_cache[-1]
            analysis['anomalies'] = anomaly_detector.detect_anomalies(latest_metrics)
        
        # æ€§èƒ½åˆ†æ
        if metrics_cache and performance_analyzer:
            latest_metrics = metrics_cache[-1]
            perf_analysis = performance_analyzer.analyze_training_efficiency(latest_metrics)
            analysis['performance'] = perf_analysis
            performance_analyzer.performance_history.append(perf_analysis)
            
            # æ€§èƒ½é¢„æµ‹
            analysis['predictions'] = performance_analyzer.predict_performance_trends()
        
        # è®¡ç®—ä¼˜åŒ–åˆ†æ•°
        if analysis['performance']:
            analysis['optimization_score'] = analysis['performance'].get('overall_score', 0)
        
        # å¥åº·è¶‹åŠ¿åˆ†æ
        if len(metrics_cache) >= 10:
            recent_scores = []
            for metrics in list(metrics_cache)[-10:]:
                if 'analysis' in metrics and 'health_score' in metrics['analysis']:
                    recent_scores.append(metrics['analysis']['health_score'])
            
            if len(recent_scores) >= 5:
                if recent_scores[-1] > recent_scores[0] + 5:
                    analysis['health_trend'] = 'improving'
                elif recent_scores[-1] < recent_scores[0] - 5:
                    analysis['health_trend'] = 'declining'
        
    except Exception as e:
        logger.error(f"æ™ºèƒ½åˆ†æå¤±è´¥: {e}")
        analysis['error'] = str(e)
    
    return analysis

# ==================== API è·¯ç”± ====================

@app.route('/')
def index():
    """APIä¿¡æ¯é¡µé¢"""
    return jsonify({
        'system': 'DLOps AIæ™ºèƒ½è¿ç»´ç³»ç»Ÿ',
        'developer': 'Charles Dong <NVIDIA HPC Engineer>',
        'version': '1.0.0',
        'status': 'running',
        'endpoints': {
            'status': '/api/status',
            'metrics': '/api/metrics', 
            'diagnostics': '/api/diagnostics',
            'intelligence': '/api/intelligence',
            'events': '/events (EventSource)',
            'daemon_control': '/api/daemon/<start|stop|restart>',
            'health_check': '/api/check',
            'auto_fix': '/api/fix/<fix_type>'
        },
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/status')
def api_status():
    """ç³»ç»ŸçŠ¶æ€API"""
    return jsonify({
        'metrics': get_latest_metrics(),
        'diagnostics': get_diagnostics(),
        'intelligence': get_intelligent_analysis(),
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/metrics')
def api_metrics():
    """æŒ‡æ ‡API"""
    return jsonify(get_latest_metrics())

@app.route('/api/diagnostics')
def api_diagnostics():
    """è¯Šæ–­API"""
    return jsonify(get_diagnostics())

@app.route('/api/intelligence')
def api_intelligence():
    """æ™ºèƒ½åˆ†æAPI"""
    return jsonify(get_intelligent_analysis())

@app.route('/api/metrics/history')
def api_metrics_history():
    """å†å²æŒ‡æ ‡API"""
    hours = request.args.get('hours', 1, type=int)
    return jsonify(list(metrics_cache)[-min(hours*20, len(metrics_cache)):])

@app.route('/api/daemon/<action>', methods=['POST'])
def api_daemon_control(action):
    """å®ˆæŠ¤è¿›ç¨‹æ§åˆ¶API"""
    if action in ['start', 'stop', 'restart']:
        result = run_dlops_command(['daemon', action])
        logger.info(f"å®ˆæŠ¤è¿›ç¨‹{action}æ“ä½œç»“æœ: {result}")
        return jsonify(result)
    return jsonify({'error': 'æ— æ•ˆæ“ä½œ'}), 400

@app.route('/api/fix/<fix_type>', methods=['POST'])
def api_auto_fix(fix_type):
    """è‡ªåŠ¨ä¿®å¤API"""
    result = run_dlops_command(['fix', fix_type])
    logger.info(f"è‡ªåŠ¨ä¿®å¤{fix_type}ç»“æœ: {result}")
    return jsonify(result)

@app.route('/api/check', methods=['POST'])
def api_health_check():
    """å¥åº·æ£€æŸ¥API"""
    result = run_dlops_command(['check'])
    logger.info(f"å¥åº·æ£€æŸ¥ç»“æœ: {result}")
    return jsonify(result)

@app.route('/events')
def events():
    """Server-Sent Eventså®æ—¶æ•°æ®æµ"""
    def event_stream():
        logger.info("EventSourceè¿æ¥å·²å»ºç«‹")
        while True:
            try:
                # è·å–å…¨é¢çŠ¶æ€
                status_data = {
                    'metrics': get_latest_metrics(),
                    'intelligence': get_intelligent_analysis(),
                    'timestamp': datetime.now().isoformat()
                }
                
                yield f"data: {json.dumps(status_data)}\n\n"
                time.sleep(3)  # 3ç§’æ›´æ–°ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"äº‹ä»¶æµé”™è¯¯: {e}")
                yield f"data: {json.dumps({'error': str(e)})}\n\n"
                time.sleep(5)
    
    return Response(event_stream(), 
                   mimetype="text/plain",
                   headers={
                       'Cache-Control': 'no-cache',
                       'Connection': 'keep-alive',
                       'Access-Control-Allow-Origin': '*'
                   })

# ==================== é™æ€æ–‡ä»¶æœåŠ¡ ====================

@app.route('/static/<path:filename>')
def serve_static(filename):
    """æä¾›é™æ€æ–‡ä»¶æœåŠ¡ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
    try:
        return send_from_directory(STATIC_DIR, filename)
    except Exception as e:
        logger.error(f"é™æ€æ–‡ä»¶æœåŠ¡å¤±è´¥: {e}")
        return jsonify({'error': 'æ–‡ä»¶æœªæ‰¾åˆ°'}), 404

# ==================== å·¥å…·å‡½æ•° ====================

def initialize_ai_components():
    """åˆå§‹åŒ–AIç»„ä»¶"""
    global anomaly_detector, performance_analyzer
    
    try:
        anomaly_detector = AnomalyDetector()
        performance_analyzer = PerformanceAnalyzer()
        logger.info("Charles Dong AIç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"AIç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")

def startup_check():
    """å¯åŠ¨æ£€æŸ¥"""
    try:
        # æ£€æŸ¥å·¥ä½œç›®å½•
        if not os.path.exists(DLOPS_WORKDIR):
            logger.warning(f"å·¥ä½œç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»º: {DLOPS_WORKDIR}")
            Path(DLOPS_WORKDIR).mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥dlopså‘½ä»¤
        dlops_script = os.path.join(DLOPS_WORKDIR, '../../../dlops')
        if not os.path.exists(dlops_script):
            dlops_script = './dlops'
        
        if not os.path.exists(dlops_script):
            logger.warning("dlopså‘½ä»¤æœªæ‰¾åˆ°ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        
        logger.info("å¯åŠ¨æ£€æŸ¥å®Œæˆ")
        
    except Exception as e:
        logger.error(f"å¯åŠ¨æ£€æŸ¥å¤±è´¥: {e}")

if __name__ == '__main__':
    # å¯åŠ¨æ£€æŸ¥
    startup_check()
    
    # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
    Path(DLOPS_WORKDIR).mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–AIç»„ä»¶
    initialize_ai_components()
    
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("â•”" + "â•" * 70 + "â•—")
    print("â•‘" + " " * 70 + "â•‘")
    print("â•‘" + "ğŸš€ DLOps AIæ™ºèƒ½è¿ç»´åç«¯æœåŠ¡".center(70) + "â•‘")
    print("â•‘" + "Developed by: Charles Dong <NVIDIA HPC Engineer>".center(70) + "â•‘")
    print("â•‘" + "AI-Driven Deep Learning Operations Backend".center(70) + "â•‘")
    print("â•‘" + " " * 70 + "â•‘")
    print("â• " + "â•" * 70 + "â•£")
    print(f"â•‘ğŸ“ APIæœåŠ¡åœ°å€: http://localhost:{WEB_PORT}".ljust(71) + "â•‘")
    print(f"â•‘ğŸ“¡ EventSource: http://localhost:{WEB_PORT}/events".ljust(71) + "â•‘")
    print(f"â•‘ğŸ“ å·¥ä½œç›®å½•: {DLOPS_WORKDIR}".ljust(71) + "â•‘")
    print(f"â•‘ğŸ“Š APIæ–‡æ¡£: http://localhost:{WEB_PORT}/".ljust(71) + "â•‘")
    print("â•‘" + " " * 70 + "â•‘")
    print("â•š" + "â•" * 70 + "â•")
    print()
    print("ğŸ§  Charles Dong AIç›‘æ§åç«¯æœåŠ¡å·²å°±ç»ª!")
    print("ğŸ“„ å‰ç«¯HTMLæ–‡ä»¶è¯·å•ç‹¬éƒ¨ç½²æˆ–ç›´æ¥æ‰“å¼€")
    print()
    
    try:
        app.run(host='0.0.0.0', port=WEB_PORT, debug=DEBUG_MODE, threaded=True)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Charles Dong's AI monitoring backend stopped")
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
